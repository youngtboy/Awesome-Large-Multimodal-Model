# Awesome-Large-Multimodal-Model
A paper list of large multimodal model (large vision-language model) 

## Survey

| **Year** | **Pub.** | **Title** |  **Links**                                       |
| :-----: | :------: | :------------------------------------ |   :----------------------------------------------------------- |
| 2023 | arxiv'23  | **Vision-Language Models for Vision Tasks:A Survey**  <br> <sub><sup>*Jingyi Zhang, Jiaxing Huang, Sheng Jin, Shijian Lu*</sup></sub> | [Paper](https://arxiv.org/pdf/2304.00685) / [Code](https://github.com/jingyi0000/VLM_survey)|
| 2023 | arxiv'23  | **A Survey on Multimodal Large Language Models**  <br> <sub><sup>*Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, Enhong Chen*</sup></sub> | [Paper](https://arxiv.org/pdf/2306.13549) / [Code](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models)|
| 2024 | arxiv'24  | **Exploring the Frontier of Vision-Language Models: A Survey of Current Methodologies and <br> Future Directions**  <br> <sub><sup>*Akash Ghosh, Arkadeep Acharya, Sriparna Saha, Vinija Jain, Aman Chadha*</sup></sub> | [Paper](https://arxiv.org/abs/2404.07214)|
| 2024 | arxiv'24  | **Efficient Multimodal Large Language Models:A Survey**  <br> <sub><sup>*Yizhang Jin, Jian Li, Yexin Liu, Tianjun Gu, Kai Wu, Zhengkai Jiang, Muyang He, Bo Zhao, Xin Tan, Zhenye Gan, Yabiao Wang, Chengjie Wang, Lizhuang Ma*</sup></sub> | [Paper](https://arxiv.org/pdf/2405.10739) / [Code](https://github.com/swordlidev/Efficient-Multimodal-LLMs-Survey)|


## 2023

| **Name** | **Pub.** | **Title** |  **Links**                                       |
| :-----: | :------: | :------------------------------------ |   :----------------------------------------------------------- |
| LLaVA | NIPS'23  | **Visual Instruction Tuning**  <br> <sub><sup>*Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee*</sup></sub> | [Paper](https://arxiv.org/abs/2304.08485) / [Code](https://github.com/haotian-liu/LLaVA) |
| Emu1 | ICLR'24  | **Emu: Generative Pretraining in Multimodality**  <br> <sub><sup>*Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, Xinlong Wang*</sup></sub>  | [Paper](https://arxiv.org/abs/2307.05222) / [Code](https://github.com/baaivision/Emu) |
| Emu2 | CVPR'24  | **Generative Multimodal Models are In-Context Learners**  <br> <sub><sup>*Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, Xinlong Wang*</sup></sub>  | [Paper](https://arxiv.org/abs/2312.13286) / [Code](https://github.com/baaivision/Emu) |
| InternVL | CVPR'24  | **InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic <br> Tasks**  <br> <sub><sup>*Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, <br>Yu Qiao, Jifeng Dai*</sup></sub>  | [Paper](https://arxiv.org/abs/2312.14238) / [Code](https://github.com/OpenGVLab/InternVL) |

## 2024

| **Name** | **Pub.** | **Title** |  **Links**                                       |
| :-----: | :------: | :----------------------------------------------------------- |   :----------------------------------------------------------- |
| EVE1 | NIPS'24  | **Unveiling Encoder-Free Vision-Language Models**  <br> <sub><sup>*Haiwen Diao, Yufeng Cui, Xiaotong Li, Yueze Wang, Huchuan Lu, Xinlong Wang*</sup></sub> | [Paper](https://arxiv.org/abs/2406.11832) / [Code](https://github.com/baaivision/EVE) |
| Emu3 | arxiv'24  | **Emu3: Next-Token Prediction is All You Need**  <br> <sub><sup>*Emu3 Team, BAAI*</sup></sub> | [Paper](https://arxiv.org/pdf/2409.18869) / [Code](https://github.com/baaivision/Emu3) |

## 2025

| **Name** | **Pub.** | **Title** |  **Links**                                       |
| :-----: | :------: | :----------------------------------------------------------- |   :----------------------------------------------------------- |
| LLaVA-mini | arxiv'25  | **LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token**  <br> <sub><sup>*Shaolei Zhang, Qingkai Fang, Zhe Yang, Yang Feng*</sup></sub> | [Paper](https://arxiv.org/abs/2501.03895) / [Code](https://github.com/ictnlp/LLaVA-Mini) |
| EVE2 | arxiv'25  | **EVEv2: Improved Baselines for Encoder-Free Vision-Language Models**  <br> <sub><sup>*Haiwen Diao, Xiaotong Li, Yufeng Cui, Yueze Wang, Haoge Deng, Ting Pan, Wenxuan Wang, Huchuan Lu, Xinlong Wang*</sup></sub> | [Paper](https://arxiv.org/abs/2502.06788) / [Code](https://github.com/baaivision/EVE) |
| Qwen2.5-VL | arxiv'25  | **Qwen2.5-VL Technical Report**  <br> <sub><sup>*Qwen Team, Alibaba Group*</sup></sub> | [Paper](https://arxiv.org/pdf/2502.13923) / [Code](https://github.com/QwenLM/Qwen2.5-VL) |
| VLM-R1 | - | **VLM-R1: A stable and generalizable R1-style Large Vision-Language Model**  <br> <sub><sup>*Om AI Lab Team*</sup></sub> | [Code](https://github.com/om-ai-lab/VLM-R1) |


