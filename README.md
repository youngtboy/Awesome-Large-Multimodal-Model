# Awesome-Large-Multimodal-Model
A paper list of large multimodal model (large vision-language model) 

## 2023

| **Name** | **Pub.** | **Title** |  **Links**                                       |
| :-----: | :------: | :----------------------------------------------------------- |   :----------------------------------------------------------- |
| LLaVA | NIPS'23  | **Visual Instruction Tuning**  <br> <sub><sup>*Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee*</sup></sub> | [Paper](https://arxiv.org/abs/2304.08485) / [Code](https://github.com/haotian-liu/LLaVA) |
| Emu1 | ICLR'24  | **Emu: Generative Pretraining in Multimodality**  <br> <sub><sup>*Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, Xinlong Wang*</sup></sub>  | [Paper](https://arxiv.org/abs/2307.05222) / [Code](https://github.com/baaivision/Emu) |

## 2024

| **Name** | **Pub.** | **Title** |  **Links**                                       |
| :-----: | :------: | :----------------------------------------------------------- |   :----------------------------------------------------------- |
| EVE1 | NIPS'24  | **Unveiling Encoder-Free Vision-Language Models**  <br> <sub><sup>*Haiwen Diao, Yufeng Cui, Xiaotong Li, Yueze Wang, Huchuan Lu, Xinlong Wang*</sup></sub> | [Paper](https://arxiv.org/abs/2406.11832) / [Code](https://github.com/baaivision/EVE) |
